\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath, amssymb, amsthm}

\newtheorem{theorem}{Theorem}

\newtheorem*{definition}{Definition}
\newtheorem*{corollary}{Corollary}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\begin{document}

\section{Introduction}

The algebraic descriptions of vector addition and scalar multiplication for vectors in a plane yield the following properties:
\begin{enumerate}
	\item For all vectors \textit{x} and \textit{y}, $x + y = y + x$.
	\item For all vectors \textit{x}, \textit{y}, and \textit{z}, $(x + y) + z = x + (y + z)$.
	\item There exists a vector denoted \textit{0} such that $x + \textit{0} = x$ for each vector \textit{x}.
	\item For each vector \textit{x} there is a vector \textit{y} such that $x + y = \textit{0}$.
	\item For each vector \textit{x}, $1x = x$.
	\item For each pair of real numbers \textit{a} and \textit{b} and each vector \textit{x}, $(ab)x = a(bx)$.
	\item For each real number \textit{a} and each pair of vectors \textit{x} and \textit{y}, $a(x + y) = ax + ay$.
	\item For each pair of real numbers \textit{a} and \textit{b} and each vector \textit{x}, $(a + b)x = ax + bx$.	
\end{enumerate}

Any mathematical structure possessing these eight properties is called a \textit{vector space}.

\section{Vector Spaces}

\begin{definition}
	A \textbf{vector space} (or \textbf{linear space}) \textbf{\textup{V}} over a field F consists of a set on which two operations (called \textbf{addition} and \textbf{scalar multiplication}, respectively) are defined so that for each pair of elements x, y, in \textbf{\textup{V}} there is a unique element $x + y$ in \textbf{\textup{V}}, and for each element a in F and each element x in \textbf{\textup{V}} there is a unique element ax in \textbf{\textup{V}}, such that the following conditions hold:
	\begin{enumerate}
		\setlength{\itemindent}{.3in}
		\item[(VS 1)] For all x, y in \textbf{\textup{V}}, $x + y = y + x$ (commutativity of addition).
		\item[(VS 2)] For all x, y, z in \textbf{\textup{V}}, $(x + y) + z = x + (y + z)$ (associativity of addition).
		\item[(VS 3)] There exists an element in \textbf{\textup{V}} denoted by 0 such that $x + \textit{0} = x$ for each x in \textbf{\textup{V}}.
		\item[(VS 4)] For each element x in \textbf{\textup{V}} there exists and element y in \textbf{\textup{V}} such that $x + y = \textit{0}$.
		\item[(VS 5)] For each element x in \textbf{\textup{V}}, $1x = x$.
		\item[(VS 6)] For each pair of elements a, b in F and each element x in \textbf{\textup{V}}, $(ab)x = a(bx)$.
		\item[(VS 7)] For each element a in F and each pair of elements x, y in \textbf{\textup{V}}, $a(x + y) = ax + ay$.
		\item[(VS 8)] For each pair of elements a, b in F and each element x in \textbf{\textup{V}}, $(a + b)x = ax + bx$.
	\end{enumerate}
	The elements x + y and ax are called the \textbf{sum} of x and y and the \textbf{product} of a and x, respectively.
\end{definition}

The elements of the field \textit{F} are called \textbf{scalars} and the elements of the vector space \textbf{V} are called \textbf{vectors}.

An object of the form $(a_1, a_2, ..., a_n)$ where the entries $a_1, a_2, ..., a_n$ are elements of a field \textit{F}, is called an \textbf{$n$-tuple} with entries from \textit{F}. The elements $a_1, a_2, ..., a_n$ are called the \textbf{entries} or \textbf{components} of the $n$-tuple. Two $n$-tuples $(a_1, a_2, ..., a_n)$ and $(b_1, b_2, ..., b_n)$ are called \textbf{equal} if $a_i = b_i$ for $i = 1, 2, ..., n$.

An $m \times n$ \textbf{matrix} with entries from a field \textit{F} is a rectangular array of the form
\[\begin{bmatrix}
	a_{11} & a_{12} & \hdots & a_{1n} \\
	a_{21} & a_{22} & \hdots & a_{2n} \\
	\vdots & \vdots & & \vdots \\
	a_{m1} & a_{m2} & \hdots & a_{mn}
\end{bmatrix}\]
where each entry $a_{ij}$ is an element of \textit{F}. We call the entries $a_{ij}$ with $i = j$ the \textbf{diagonal entries} of the matrix.

The $m \times n$ matrix in which each entry equals zero is called the \textbf{zero matrix} and is denoted by \textit{O}.

If the number of rows and columns of a matrix are equal, the matrix is called \textbf{square}.

A \textbf{polynomial} with coefficients from a field \textit{F} is an expression of the form \[f(x) = a_nx^n + a_{n-1}x^{n-1} + \hdots + a_1x + a_0,\] where $n$ is a nonnegative integer and each $a_k$, called the \textbf{coefficient} of $x^k$, is in \textit{F}. If $f(x) = 0$, then $f(x)$ is called the \textbf{zero polynomial} and, for convenience, its degree is defined to be $-1$; otherwise, the \textbf{degree} of a polynomial is defined to be the largest exponent of $x$ that appears in the representation with a nonzero coefficient.

\begin{theorem}[\textbf{Cancellation Law for Vector Addition}]
	If $x$, $y$, and $z$ are vectors in a vector space \textbf{\textup{V}} such that $x + z = y + z$ then $x = y$.
\end{theorem}

The vector \textit{0} in (VS 3) is called the \textbf{zero vector} of \textbf{V}, and the vector $y$ in (VS 4) is called the \textbf{additive inverse} of $x$ and is denoted by $-x$.

\begin{theorem}
	In any vector space \textbf{\textup{V}}, the following statements are true:
	\begin{enumerate}
		\item[(a)] $0x = 0$ for each $x\in\textbf{\textup{V}}$.
		\item[(b)] $(-a)x = -(ax) = a(-x)$ for each $a\in F$ and each $x\in\textbf{\textup{V}}$.
		\item[(c)] $a\textit{0} = \textit{0}$ for each $a\in F$.
	\end{enumerate}
\end{theorem}

\pagebreak
\subsection*{Exercises}
\subsubsection*{1. Label the following statements as true or false.}
\begin{enumerate}
	\item[(a)] Every vector space contains a zero vector - \textbf{True}.
	
	It is included in the definition of a vector space (VS 3).
	\item[(b)] A vector space may have more than one zero vector - \textbf{False}.
	
	Suppose there were two such vectors, $x$ and $y$, and one nonzero vector $z$. Then $x + z = z = y + z$, and $x + (z + (-z)) = x = y + (z + (-z)) = y$. 
	\item[(c)] In any vector space, $ax = bx$ implies that $a = b$ - \textbf{False}.
	
	Consider $x = \textit{0}$ but $a \ne b$.
	\item[(d)] In any vector space, $ax = ay$ implies that $x = y$ - \textbf{False}.
	
	Consider $a = 0$ but $x \ne y$.
	\item[(e)] A vector in $F^n$ may be regarded as a matrix in $M_{n\times 1}(F)$ - \textbf{True}.
	\item[(f)] An $m \times n$ matrix has $m$ columns and $n$ rows - \textbf{False}.
	
	An $m \times n$ matrix has $m$ rows and $n$ columns.
	\item[(g)] In $P(F)$, only polynomials of the same degree may be added - \textbf{False}.
	
	Not true based on the definition of addition in $P(F)$.
	\item[(h)] If $f$ and $g$ are polynomials of degree $n$, then $f + g$ is a polynomial of degree $n$ - \textbf{False}.
	
	Consider $x$ and $-x$.
	\item[(i)] If $f$ is a polynomial of degree $n$ and $c$ is a nonzero scalar, then $cf$ is a polynomial of degree $n$ - \textbf{True}.
	
	Follows from definition of scalar multiplication in $P(F)$.
	\item[(j)] A nonzero scalar of \textit{F} may be considered to be a polynomial in $P(F)$ having degree zero - \textbf{True}.
	
	If $a$ is a nonzero scalar, it can be expressed as $ax^0$.
	\item[(k)] Two functions in $\mathcal{F}(S, F)$ are equal if and only if the have the same value at each element of $S$ - \textbf{True}.
	
	By definition, two functions $f$, $g$ in $\mathcal{F}(S, F)$ are equal when $f(x) = g(x)$ for each $x$ in F.
\end{enumerate}

\subsubsection*{2. Write the zero vector of $M_{3 \times 4}(F)$.}
\[\begin{bmatrix}
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0 \\
	0 & 0 & 0 & 0
\end{bmatrix}\]

\subsubsection*{8. In any vector space \textbf{V}, show that $(a + b)(x + y) = ax + ay + bx + by$ for any $x, y \in \textbf{\textup{V}}$ and any $a, b \in F$.}
$(a + b)(x + y) = (a + b)x + (a + b)y = ax + bx + ay + by$.

\subsubsection*{9. Prove Corollaries 1 and 2 of Theorem 1.1 and Theorem 1.2(c).}
\begin{corollary}
	The vector 0 described in (VS 3) is unique.
\end{corollary}
\begin{proof}
	Suppose that there are vectors $x, y, z \in \textbf{\textup{V}}$ such that $x + z = y + z = z$. Then $x = x + 0 = x + (z + (-z)) = (x + z) + (-z) = (y + z) + (-z) = y + (z + (-z)) = y + 0 = y$.
\end{proof}
\begin{corollary}
	The vector y described in (VS 4) is unique.
\end{corollary}
\begin{proof}
	Suppose that there are vectors $x, y, z \in \textbf{\textup{V}}$ such that $x + y = x + z = 0$. Then $y = 0 + y = x + (-x) + y = (x + y) + (-x) = (x + z) + (-x) = x + (-x) + z = 0 + z = z$.
\end{proof}

\subsubsection*{11. Let $\textbf{\textup{V}} = \{\textit{0}\}$ consist of a single vector \textit{0} and define $\textit{0} + \textit{0} = \textit{0}$ nad $c\textit{0} = \textit{0}$ for each scalar $c$ in $F$. Prove that \textbf{V} is a vector space over $F$. (\textbf{V} is called the \textbf{zero vector space}.)}
\begin{proof}
	For any $x,y,z \in \textbf{\textup{V}}$ and $a,b \in F$:
	\begin{enumerate}
		\item $x + y = 0 + 0 = y + x$ (VS 1)
		\item $(x + y) + z = (0 + 0) + 0 = 0 + (0 + 0) = x + (y + z)$ (VS 2)
		\item $x + 0 = 0 + 0 = 0 = x$ (VS 3)
		\item $x + y = 0 + 0 = 0$ (VS 4)
		\item $1x = 1 \times 0 = 0 = x$ (VS 5)
		\item $(ab)x = (ab) \times 0 = 0 = a (b \times 0) = a(bx)$ (VS 6)
		\item $a(x + y) = a(0 + 0) = 0 + 0 = a \times 0 + a \times 0 = ax + ay$ (VS 7)
		\item $(a + b)x = (a + b) \times 0 = 0 = 0 + 0 = a \times 0 + b \times 0 = ax + bx$ (VS 8)
	\end{enumerate}
	Therefore \textbf{V} satisfies all conditions necessary for it to be a vector space.
\end{proof}

\pagebreak

\subsubsection*{13. Let \textbf{V} denote the set of ordered pairs of real numbers. If $(a_1, a_2)$ and $(b_1, b_2)$ are elements of \textbf{V} and $c \in R$, define \[(a_1, a_2) + (b_1, b_2) = (a_1 + b_1, a_2b_2)\] and \[c(a_1, a_2) = (ca_1, a_2).\] Is \textbf{V} a a vector space over $R$ with these operations?}
\begin{proof}
	Let $(x_1, x_2) \in \textbf{\textup{V}}$ and $a,b \in R$. Then \[(a + b)(x_1, x_2) = ((a + b)x_1, x_2) = (ax_1 + bx_1, x_2)\]and \[a(x_1, x_2) + b(x_1, x_2) = (ax_1, x_2) + (bx_1, x_2) = (ax_1 + bx_1, x_2^2)\]so \[(a + b)(x_1, x_2) \ne a(x_1, x_2) + b(x_1, x_2)\]so \textbf{V} is not a vector space over $R$.
\end{proof}

\subsubsection*{14. Let $\textbf{\textup{V}} = \{(a_1, a_2, \dots, a_n): a_i \in C \text{ for } i=1,2,\dots,n$\}; so \textbf{V} is a vector space over $C$ by Example 1. Is \textbf{V} a vector space over the field of real numbers with the operations of coordinatewise addition and multiplication?}
\begin{proof}
	Notice that any number $x \in R$ can be expressed as $x + 0i$ in $C$, so if \textbf{V} is a vector space over $C$, it is also a vector space over $R$.
\end{proof}

\subsubsection*{15. Let $\textbf{\textup{V}} = \{(a_1, a_2, \dots, a_n): a_i \in R \text{ for } i=1,2,\dots,n\}$; so \textbf{V} is a vector space over $R$ by Example 1. Is \textbf{V} a vector space over the field of complex numbers with the operations of coordinatewise addition and scalar multiplication?}
\begin{proof}
	Consider $c = x + yi$ and $a = (a_1)$ with $y,a_1 \ne 0$. Then $ca = (x + yi)(a_1) = (xa_1 + ya_1i)$, so the entries of ca aren't in $R$, so \textbf{V} is not a vector space over $C$.
\end{proof}

\subsubsection*{17. Let $\textbf{V} = \{(a_1,a_2):a_1,a_2 \in F\}$, where $F$ is a field. Define the addition of elements of \textbf{V} coordinatewise, and for $c \in F$ and $(a_1,a_2) \in \textbf{V}$, define \[c(a_1, a_2) = (a_1, 0).\] Is \textbf{V} a vector space over $F$ with these operations?}
\begin{proof}
	Consider $(a_1,a_2) \in \textbf{\textup{V}}$ with $a_2 \ne 0$. Then $1(a_1, a_2) = (a_1, 0)$, so $1(a_1, a_2) \ne (a_1, a_2)$, therefore \textbf{V} is not a vector space over $F$.
\end{proof}

\subsubsection*{18. Let $\textbf{\textup{V}} = \{(a_1, a_2):a_1,a_2 \in R\}$. For $(a_1, a_2),(b_1, b_2) \in \textbf{\textup{V}}$ and $c \in R$, define \[(a_1, a_2) + (b_1, b_2) = (a_1 + 2b_1, a_2 + 3b_2) \text{ and } c(a_1, a_2) = (ca_1, ca_2).\] Is \textbf{V} a vector space over $R$ with these operations?}
\begin{proof}
	Consider $(a_1, a_2) = (1, 1)$ and $(b_1, b_2) = (2, 2)$. Then $(a_1, a_2) + (b_1, b_2) = (5, 7)$ and $(b_1, b_2) + (a_1, a_2) = (4, 5)$, so $(a_1, a_2) + (b_1, b_2) \ne (b_1, b_2) + (a_1, a_2)$, therefore \textbf{V} is not a vector space over $R$.
\end{proof}

\subsubsection*{21. Let \textbf{V} and \textbf{W} be vector spaces over a field $F$. Let \[\textbf{\textup{Z}} = \{(v,w):v \in \textbf{\textup{V}} \text{ and } w \in \textbf{\textup{W}}\}.\] Prove that \textbf{Z} is a vector space over $F$ with the operations \[(v_1, w_1) + (v_2, w_2) = (v_1 + v_2, w_1 + w_2) \text{ and } c(v_1, w_1) = (cv_1, cw_1).\]}
\begin{proof}
	For $(v_1, w_1), (v_2, w_2), (v_3, w_3) \in \textbf{\textup{Z}}$ and $a,b \in F$
	\begin{enumerate}
		\item $(v_1, w_1) + (v_2, w_2) = (v_1 + v_2, w_1 + w_2) = (v_2 + v_1, w_2 + w_1) = (v_2, w_2) + (v_1, w_1)$ (VS 1)
		\item $((v_1, w_1) + (v_2, w_2)) + (v_3, w_3) = (v_1 + v_2, w_1 + w_2) + (v_3, w_3) = (v_1 + v_2 + v_3, w_1 + w_2 + w_3) = (v_1, w_1) + (v_2 + v_3, w_2 + w_3) = (v_1, w_1) + ((v_2, w_2) + (v_3, w_3))$ (VS 2)
		\item $(v_1, w_1) + 0 = (v_1, w_1) + (0, 0) = (v_1 + 0, w_1 + 0) = (v_1, w_1)$ (VS 3)
		\item $(v_1, w_1) + (-v_1, -w_1) = (v_1 - v_1, w_1 - w_1) = (0, 0)$ (VS 4)
		\item $1(v_1, w_1) = (1v_1, 1w_1) = (v_1, w_1)$ (VS 5)
		\item $(ab)(v_1, w_1) = (abv_1, abw_1) = a(bv_1, bw_1) = a(b(v_1, w_1))$ (VS 6)
		\item $a((v_1, w_1) + (v_2, w_2)) = a(v_1 + v_2, w_1 + w_2) = (av_1 + av_2, aw_1 + aw_2) = (av_1, aw_1) + (av_2, aw_2) = a(v_1, w_1) + a(v_2, w_2)$ (VS 7)
		\item $(a + b)(v_1, w_1) = ((a+b)v_1, (a+b)w_1) = (av_1 + bv_1, aw_1 + bw1) = (av_1, aw_1) + (bv_1, bw_1) = a(v_1, w_1) + b(v_1, w_1)$ (VS 8)
	\end{enumerate}
\end{proof}

\section{Subspaces}
\begin{definition}
	A subset \textbf{\textup{W}} of a vector space \textbf{\textup{V}} over a field $F$ is called a \textbf{subspace} of \textbf{\textup{V}} if \textbf{\textup{W}} is a vector space over $F$ with the operations of addition and scalar multiplication defined on \textbf{\textup{V}}.
\end{definition}

\begin{theorem}
	Let \textbf{\textup{V}} be a vector space and \textbf{\textup{W}} a subset of \textbf{\textup{V}}. Then \textbf{\textup{W}} is a subspace of \textbf{\textup{V}} if and only if the following three conditions hold for the operations defined in \textbf{\textup{V}}.
	\begin{enumerate}
		\item $0 \in \textbf{\textup{W}}$.
		\item $x + y \in \textbf{\textup{W}}$ whenever $x \in \textbf{\textup{W}}$ and $y \in \textbf{\textup{W}}$.
		\item $cx \in \textbf{\textup{W}}$ whenever $c \in F$ and $x \in \textbf{\textup{W}}$.
	\end{enumerate}
\end{theorem}

The \textbf{transpose} $A^t$ of an $m \times n$ matrix $A$ is the $n \times m$ matrix obtained from $A$ by interchanging the rows with the columns; that is $(A^t)_{ij} = A_{ji}$.

A \textbf{symmetric} matrix is a matrix $A$ such that $A^t = A$.

An $m \times n$ matrix $A$ is called \textbf{upper triangular} if all its entries lying below the diagonal entries are zero, that is, $A_{ij} = 0$ whenever $i > j$. An $n \times n$ matrix $M$ is called a \textbf{diagonal matrix} if $M_{ij} = 0$ whenever $i \ne j$, that is, if all its nondiagonal entries are zero.

The \textbf{trace} of an $n \times n$ matrix $M$, denoted $\text{tr}(M)$, is the sum of diagonal entries of $M$; that is \[ \text{tr}(M) = M_{11} + M_{22} + \dots + M_{nn} \]

\begin{theorem}
	Any intersection of subspaces of a vector space \textbf{\textup{V}} is a subspace of \textbf{V}.
\end{theorem}

\begin{definition}
	If $S_1$ and $S_2$ are nonempty subsets of a vector space $V$, then the \textbf{sum} of $S_1$ and $S_2$, denoted $S_1 + S_2$, is the set $\left\{x + y: x\in S_1 \text{ and } y \in S_2\right\}.$
\end{definition}

\begin{definition}
	A vector space $V$ is called the \textbf{direct sum} of $W_1$ and $W_2$ if $W_1$ and $W_2$ are subspaces of $V$ such that $W_1 \cap W_2 = {0}$ and $W_1 + W_2 = V$. We denote that $V$ is the direct sum of $W_1$ and $W_2$ by writing $V = W_1 \bigoplus W_2$.
\end{definition}

\subsection*{Exercises}
\subsubsection*{1. Label the following statements as true or false.}
\begin{enumerate}
	\item[(a)] If \textbf{V} is a vector space and \textbf{W} is a subset of \textbf{V} that is a vector space, then \textbf{W} is a subspace of \textbf{V} - \textbf{True}.

	This is the definition of a subspace.
	\item[(b)] The empty set is a subspace of every vector space - \textbf{False}.
	
	The empty set does not contain 0, which is necessary for it to be a subspace.
	\item[(c)] If \textbf{V} is a vector space other than the zero vector space, then \textbf{V} contains a subspace \textbf{W} such that $\textbf{\textup{W}} \ne \textbf{\textup{V}}$ - \textbf{True}.
	
	The zero subspace fulfills this condition.
	\item[(d)] The intersection of any two subsets of \textbf{V} is a subspace of \textbf{V} - \textbf{False}.
	
	If neither subset contains 0, then their intersection can not be a subspace.
	\item[(e)] An $n \times n$ diagonal matrix can never have more than $n$ nonzero entries - \textbf{True}.
	
	All non-diagonal entries of a diagonal matrix are always 0, and an $n \times n$ matrix has $n$ diagonal entries.
	\item[(f)] The trace of a square matrix is the product of its diagonal entries - \textbf{False}.
	
	The trace of a square matrix is the sum of its diagonal entries.
	\item[(g)] Let \textbf{W} be the $xy$-plane in $R^3$; that is, $\textbf{\textup{W}} = \left\{(a_1, a_2, 0): a_1, a_2 \in R\right\}$. Then $\textbf{\textup{W}} = R^2$ - \textbf{False}.
	
	$R^2 = \left\{(a_1, a_2): a_1,a_2 \in R\right\}$, so $\textbf{\textup{W}} \ne R^2$.
\end{enumerate}

\subsubsection*{3. Prove that $(aA + bB)^t = aA^t + bB^t$ for any $A,B \in \textbf{\textup{M}}_{n \times n}(F)$ and any $a, b \in F$.}
\begin{proof}
	\begin{align*}
		(aA^t + bB^t)_{ij} = {aA^t}_{ij} + {bB^t}_{ij} &= aA_{ji} + bB_{ji} = (aA+bB)_{ji} = {(aA+bB)^t}_{ij},\\& \text{ so } (aA^t + bB^t) = (aA + bB)^t.
	\end{align*}
\end{proof}

\subsubsection*{4. Prove that $(A^t)^t = A$ for each $A \in \textbf{\textup{M}}_{m\times n}(F)$.}
\begin{proof}
	\[ {(A^t)^t}_{ij} = {A^t}_{ji} = A_{ij} \text { so } {(A^t)^t} = A. \]
\end{proof}

\subsubsection*{5. Prove that $A + A^t$ is symmetric for any square matrix $A$.}
\begin{proof}
	Let $A$ be an $n \times n$ square matrix. Then \[ A_{ij} + {A^t}_{ij} = A_{ij} + A_{ji} = A_{ji} + A_{ij} = A_{ji} + {A^t}_{ji}, \] so ${(A+A^t)_{ij} = (A+A^t)_{ji} = {(A+A^t)^t}_{ij}}$, therefore $A+A^t$ is symmetric.
\end{proof}

\subsubsection*{6. Prove that $\text{tr}(aA + bB) = a\text{tr}(A) + b\text{tr}(B)$ for any $A,B \in \textbf{\textup{M}}_{n \times n}(F)$.}
\begin{proof}
	For any $i \le n$, $(aA + bB)_{ii} = (aA)_{ii} + (bB)_{ii} = a(A_{ii}) + b(B_{ii})$, so $\text{tr}(aA + bB) = a\text{tr}(A) + b\text{tr}(B)$.
\end{proof}

\subsubsection*{7. Prove that diagonal matrices are symmetric matrices.}
\begin{proof}
	Let $A$ be an $n \times n$ diagonal matrix. Then for any $i, j \le n$ such that $i \ne j$, $A_{ij} = 0 = A_{ji} = {A^t}_{ij}$. If $i = j$, then $A_{ij} = A_{ji} = {A^t}_{ij}$, so the matrix is symmetric.
\end{proof}

\subsubsection*{8. Determine whether the following sets are subspaces of $R^3$ under the operations of addition and scalar multiplication defined on $R^3$. Justify your answers}
\begin{enumerate}
	\item[(a)] $W_1 = \left\{(a_1, a_2, a_3) \in R^3: a_1 = 3a_2\text{ and }a_3 = -a_2\right\}$.
	
	$0$ is in $W_1$. Let $x = (3x_2, x_2, -x_2)$ and $y = (3y_2, y_2, -y_2)$. Then $x + y = (3x_2 + 3y_2, x_2 + y_2, -x_2 - y_2) = (3(x_2 + y_2), x_2 + y_2, -(x_2 + y_2))$, so $x + y \in W_1$. Finally let $c \in R$. Then $cx = (3cx_2, cx_2, -cx_2)$, so $cx \in W_1$, therefore $W_1$ is a subspace of $R^3$.
	\item[(b)] $W_2 = \left\{(a_1, a_2, a_3) \in R^3: a_1 = a_3 + 2\right\}$.
	
	$0 \notin W_2$, so $W_2$ is not a subspace of $R^3$.
	\item[(c)] $W_3 = \left\{(a_1, a_2, a_3) \in R^3: 2a_1 - 7a_2 + a_3 = 0\right\}$
	
	$0 \in W_3$. Let $x = (x_1, x_2, 7x_2 - 2x_1)$ and $y = (y_1, y_2, 7y_2 - 2y_1)$. Then $x + y = (x_1 + y_1, x_2 + y_2, 7x_2 - 2x_1 + 7y_2 - 2y_1) = (x_1 + y_1, x_2 + y_2, 7(x_2 + y_2) - 2(x_1 + y_1))$, so $x + y \in W_3$. Finally, let $c \in R$. Then $cx = (cx_1, cx_2, c(7x_2 - 2x_1)) = (cx_1, cx_2, 7cx_2 - 2cx_1) \in W_3$, so $W_3$ is a subspace of $R^3$.
	\item[(d)] $W_4 = \left\{(a_1, a_2, a_3) \in R^3: a_1 - 4a_2 - a_3 = 0\right\}$
	
	$0 \in W_4$. Let $x = (4x_2 + x_3, x_2, x_3)$ and $y = (4y_2 + y_3, y_2, y_3)$. Then $x + y = (4x_2 + x_3 + 4y_2 + y_3, x_2 + y_2, x_3 + y_3) = (4(x_2 + y_2) + (x3 + y_3), x_2 + y_2, x_3 + y_3) \in W_4$. Finally let $c \in R$. Then $cx = (c(4x_2 + x_3), cx_2, cx_3) = (4cx_2 + cx_3, cx_2, cx_3) \in W_4$, so $W_4$ is a subspace of $R^3$.
	\item[(e)] $W_5 = \left\{(a_1, a_2, a_3) \in R^3: a_1 + 2a_2 - 3a_3 = 1\right\}$
	
	$0 \notin W_5$, so $W_5$ is not a subspace of $R^3$.
	\item[(f)] $W_6 = \left\{(a_1, a_2, a_3) \in R^3:5{a_1}^2 - 3{a_2}^2 + 6{a_3}^2 = 0\right\}$
	
	Let $x = \left(\sqrt{\frac{3}{5}{x_2}^2 - \frac{6}{5}{x_3}^2}, x_2, x_3\right)$ and $c \in R$. Then $cx = \left(c\sqrt{\frac{3}{5}{x_2}^2 - \frac{6}{5}{x_3}^2}, cx_2, cx_3\right) = (\sqrt{\frac{3}{5}c^2{x_2}^2 - \frac{6}{5}c^2{x_3}^2}) \notin W_6$, so $W_6$ is not a subspace of $R^3$.
\end{enumerate}

\subsubsection*{9. Let $W_1, W_3 \text{ and } W_4$ be as in exercise 8. Describe $W_1 \cap W_3$, $W_1 \cap W_4$, and $W_3 \cap W_4$, and observe that each is a subspace of $R^3$.}
\begin{proof}
	$W_1 \cap W_3 = \left\{(a_1, a_2, a_3) \in R^3: a_1 = 3a_2, a_2 = \frac{1}{4}a_1,\text{ and } a_3 = 7a_2 - 2a_1\right\}$. We have $a_1 = 3a_2$ and $4a_2 = a_1$, so $a_1 = a_2 = 0$ always holds. Then $a_3 = 7a_2 - 2a_1 = 0$, so $W_1 \cap W_3$ is the zero subspace of $R^3$.
	
	$W_1 \cap W_4 = \left\{(a_1, a_2, a_3) \in R^3: a_1 = 4a_2-a_3, a_2 = \frac{1}{3}a_1, \text{ and } a_3 = -a_2\right\}$. We have $a_2 = \frac{1}{3}a_1 = \frac{4}{3}a_2 - \frac{1}{3}a_3$, so $0 = \frac{1}{3}(a_2 - a_3)$, and thus $a_3 = a_2$. Since $a_3 = a_2$ and $a_3 = -a_2$, $a_3 = 0 = a_2$, and then $a_1 = 4a_2 - a_3 = 0$, so $W_1 \cap W_4$ is the zero subspace of $R^3$.

	$W_3 \cap W_4 = \left\{(a_1, a_2, a_3) \in R^3: 2a_1 - 7a_2 + a_3 = 0 \text{ and } a_1 - 4a_2 -a_3 = 0\right\}$. We then have $a_1 = \frac{11}{3}a_2$ and $a_3 = 7a_2 - 2a_1 = 7a_2 - \frac{22}{3}a_2 = -\frac{1}{3}a_2$. Then $0 \in W_3 \cap W_4$. Let $x = (\frac{11}{2}x_2, x_2, -\frac{1}{3}x_2)$ and $y = (\frac{11}{3}y_2, y_2, -\frac{1}{3}y_2)$. Then $x + y = (\frac{11}{3}(x_2 + y_2), x_2 + y_2, -\frac{1}{3}(x_2 + y_2)) \in W_3 \cap W_4$. Let $c \in R$. Then $cx = (\frac{11}{3}cx_2, cx_2, -\frac{1}{3}cx_2) \in W_3 \cap W_4$, so $W_3\cap W_4$ is a subspace of $R^3$.
\end{proof}

\subsubsection*{11. Is the set $W = \left\{f(x) \in P(F): f(x) = 0 \text{ or } f(x) \text{ has degree }n\right\}$ a subspace of $P(F)$ if $n \ge 1$? Justify your answer.}
\begin{proof}
	Let $f(x) = x^2 + x$ and $g(x) = -x^2 + x$. Then $n = 2$, and $f(x) + g(x) = 2x$, so $W$ is not closed under addition, and therefore it is not a subspace of $P(F)$.
\end{proof}

\subsubsection*{12. Prove that the set of $m \times n$ upper triangular matrices is a subspace of $M_{m\times n}(F)$.}
\begin{proof}
	Let $W$ be the set of $m \times n$ upper triangular matrices. Then $0 \in W$. Let $x,y \in W$. Then $x + y \in W$, since for all $i,j$ such that $i > j$, $(x+y)_{ij} = 0 + 0 = 0$. Let $c \in F$. Then $cx \in W$, since for all $i,j$ such that $i > j$, $(cx)_{ij} = c \cdot 0 = 0$, so $W$ is a subspace of $M_{m\times n}(F)$.
\end{proof}

\subsubsection*{17. Prove that a subset $W$ of a vector space $V$ is a subspace of $V$ if and only if $W \ne \emptyset$ and, whenever $a \in F$ and $x, y \in W$, then $ax \in W$ and $x + y \in W$.}
\begin{proof}
	Suppose that $W$ is a subset of a vector space $V$. If $W$ is a subspace of $V$, then it must contain $0$, so $W \ne \emptyset$, and for $a \in F$ and $x,y \in W$, $x + y \in W$ and $ax \in W$ is true by theorem 1.3. For the converse, suppose that $W \ne \emptyset$, and for $a \in F$ and $x,y \in W$, $x + y \in W$ and $ax \in W$. Then for $a = 0$, $ax = 0 \in W$, so $W$ satisfies all conditions necessary for it to be a subspace of $V$.
\end{proof}

\subsubsection*{18. Prove that a subset $W$ of a vector space $V$ is a subspace of $V$ if and only if $0 \in W$ and $ax + y \in W$ whenever $a \in F$ and $x,y \in W$.}
\begin{proof}
	Let $W$ be a subset of a vector space $V$. If $W$ is a subspace of $V$, then it has to contain $0$, and for $a \in F$ and $x, y\in W$, $ax \in W$ and $x + y \in W$. If $z = ax$, then $z + y \in W$ has to be true.

	For the converse, suppose that $0 \in W$ and for $a \in F$ and $x,y \in W$, $ax + y \in W$. If $y = 0$, then $ax + y = ax \in W$. Since $ax \in W$, let $z = ax$, so $z + y \in W$, which means that $W$ is a subspace of $V$.
\end{proof}

\subsubsection*{20. Prove that if $W$ is a subspace of a vector space $V$ and $w_1, w_2, \dots, w_n$ are in $W$, then $a_1w_1 + a_2w_2 + \dots + a_nw_n \in W$ for any scalars $a_1, a_2, \dots, a_n$.}
\begin{proof}
	Let $n = 1$. Then $a_1w_1 \in W$ by the definition of a subspace. Next, suppose that for some $n \ge 2$, $a_1w_1 + a_2w_2 + \dots + a_{n-1}w_{n-1} \in W$. Then \[ a_1w_1 + a_2w_2 + \dots a_nw_n = (a_1w_1 + a_2w_2 + \dots a_{n-1}w_{n-1}) + a_nw_n. \] Since $(a_1w_1 + a_2w_2 + \dots + a_{n-1}w_{n-1}) \in W$ and $a_nw_n \in W$ and subspaces are closed under addition, $a_1w_1 + a_2w_2 + \dots + a_nw_n \in W$.
\end{proof}

\subsubsection*{23. Let $W_1$ and $W_2$ be subspaces of a vector space $V$.}
\begin{enumerate}
	\item[(a)] Prove that $W_1 + W_2$ is a subspace of $V$ that contains both $W_1$ and $W_2$.
	
	Assume, without loss of generality that $x$ is a vector in $W_1$. Then, since $W_2$ contains $0$, $x + 0 = x \in W_1 + W_2$.
	\item[(b)] Prove that any subspace of $V$ that contains both $W_1$ and $W_2$ must also contain $W_1 + W_2$.
	
	Let $W_3$ be a subspace of $V$, such that it contains both $W_1$ and $W_2$. Since subspaces are closed under addition, for any $x \in W_1$ and $y \in W_2$, $x + y \in W_3$, so $W_3$ must contain $W_1 + W_2$.
\end{enumerate}

\subsubsection*{24. Show that $F^n$ is the direct sum of the subspaces \[ W_1 = \left\{(a_1, a_2, \dots, a_n) \in F^n: a_n = 0\right\} \] and \[ W_2 = \left\{(a_1, a_2, \dots, a_n) \in F^n: a_1 = a_2 = \dots = a_{n-1} = 0\right\}. \]}
\begin{proof}
	Let $x = (x_1, x_2, \dots, x_n) \in W_1$ and $y = (y_1, y_2, \dots, y_n) \in W_2$. Then \[ x + y = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n) = (x_1 + 0, x_2 + 0, \dots, x_{n-1} + 0, 0 + y_n) = (x_1, x_2, \dots, y_n), \]
	where $x_1, x_2, \dots, x_{n-1}, y_n \in F$, so $F^n = W_1 + W_2$.

	Next, \[ W_1 \cap W_2 = \left\{(a_1, a_2, \dots, a_n) \in F^n: a_1 = a_2 = \dots = a_n = 0\right\} = \left\{0\right\},\] so $F^n = W_1 \bigoplus W_2$.
\end{proof}

\subsubsection*{27. Let $V$ denote the vector space of all upper triangular $n \times n$ matrices, and let $W_1$ denote the subspace of $V$ consisting of all diagonal matrices. Define \[W_2 = \left\{A \in V: A_{ij} = 0 \text{ whenever } i \ge j\right\}.\] Show that $V = W_1 \bigoplus W_2$.}
\begin{proof}
	Let $A \in W_1$ and $B \in W_2$. Then
	\begin{align*}
		A + B = 
		\begin{bmatrix}
			a_{11} & 0 & \hdots & 0 \\
			0 & a_{22} & \hdots & 0 \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \hdots & a_{nn}
		\end{bmatrix} +
		\begin{bmatrix}
			0 & b_{12} & \hdots & b_{1n} \\
			0 & 0 & \hdots & b_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \hdots & 0
		\end{bmatrix} =
		\begin{bmatrix}
			a_{11} & b_{12} & \hdots & b_{1n} \\
			0 & a_{22} & \hdots & b_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & \hdots & a_{nn}
		\end{bmatrix},
	\end{align*}
	so $V = W_1 + W_2$.

	Next, \[ W_1 \cap W_2 = \left\{A \in V: A_{ij} = 0 \text{ whenever } i \ge j \text{ or } i \ne j\right\} = \left\{0\right\}, \]
	so $V = W_1 \bigoplus W_2$.
\end{proof}

\subsubsection*{28. A matrix $M$ is called \textbf{skew-symmetric} if $M^t = -M$. Clearly, a skew-symmetric matrix is square. Let $F$ be a field. Prove that the set $W_1$ of all skew-symmetric $n \times n$ matrices with entries from $F$ is a subspace of $M_{n\times n}(F)$. Now assume that $F$ is not of characteristic two, and let $W_2$ be the subspace of $M_{n\times n}(F)$ consisting of all symmetric $n \times n$ matrices. Prove that $M_{n \times n}(F) = W_1 \bigoplus W_2$.}
\begin{proof}
	$0 \in W_1$, since $0^t = 0 = -0$. Let $A, B \in W_1$. Then \[(A + B)^t = A^t + B^t = -A + (-B) = -(A + B),\] so $A + B \in W_1$. Now let $a \in F$. Then $(aA)^t = aA^t = -aA$, so $aA \in W_1$, and thus $W_1$ is a subspace of $M_{n \times n}(F)$.

	Let $A$ be any square $n \times n$ matrix. Then if $B$ is an $n \times n$ square matrix, such that $B_{ij} = A_{ij}$ whenever $i = j$, $B_{ij} = A_{ij} + \frac{1}{2}(A_{ji} - A_{ij})$ whenever $i < j$ and $B_{ij} = B_{ji}$ whenever $i > j$, then $B$ is a symmetric matrix, so $B \in W_2$, and if $C$ is an $n \times n$ square matrix, such that $C_{ij} = 0$ whenever $i = j$, $C_{ij} = \frac{1}{2}(A_{ji} - A_{ij})$ whenever $i > j$ and $C_{ij} = -C_{ji}$ whenever $i < j$, then $C$ is skew-symmetric, so $C \in W_1$. Then:
	\begin{enumerate}
		\item If $i = j$ \[ B_{ij} + C_{ij} = A_{ij} + 0 = A_{ij} \]
		\item If $i < j$ \[ B_{ij} + C_{ij} = A_{ij} + \frac{1}{2}(A_{ji} - A_{ij}) - \frac{1}{2}(A_{ji} - A_{ij}) = A_{ij} \]
		\item If $j > i$ \[ B_{ij} + C_{ij} = A_{ji} + \frac{1}{2}(A_{ij} - A_{ji}) + \frac{1}{2}(A_{ij} - A_{ji}) = A_{ji} + A_{ij} - A_{ji} = A_{ij}, \]
	\end{enumerate}
	so $A = B + C$, and thus $M_{n \times n}(F) = W_1 + W_2$.
	\[ W_1 \cap W_2 = \left\{A \in M_{n \times n}(F): A^t = -A \text{ and } A^t = A\right\} = \left\{0\right\}, \] so $M_{n \times n}(F) = W_1 \bigoplus W_2$.
\end{proof}

\subsubsection*{30. Let $W_1$ and $W_2$ be subspaces of a vector space $V$. Prove that $V$ is the direct sum of $W_1$ and $W_2$ if and only if each vector in $V$ can be uniquely written as $x_1 + x_2$ where $x_1 \in W_1$ and $x_2 \in W_2$.}
\begin{proof}
	Suppose that $V$ is the direct sum of $W_1$ and $W_2$, and that there exist some $x_1, {x_1}^\prime \in W_1$ and $x_2, {x_2}^\prime \in W_2$, such that $x_1 + x_2 = {x_1}^\prime + {x_2}^\prime \in V$. Then $x_1 - {x_1}^\prime = {x_2}^\prime - x_2 \in W_1 \cap W_2 = \left\{0\right\}$, so $x_1 - {x_1}^\prime = 0 = {x_2}^\prime - x_2$, and $x_1 = {x_1}^\prime$ and $x_2 = {x_2}^\prime$.

	For the converse, suppose that any vector in $V$ can be represented uniquely as $x_1 + x_2$, for $x_1 \in W_1$ and $x_2 \in W_2$. Then $V = W_1 + W_2$. Let $y \in W_1 \cap W_2$. Then $y = y + 0 = 0 + y$, and since it is unique, $y = 0$, so $W_1 \cap W_2 = \left\{0\right\}$.
\end{proof}

\section{Linear combinations and systems of linear equations}

\begin{definition}
	Let $V$ be a vector space and $S$ a nonempty subset of $V$. A vector $v \in V$ is called a \textbf{linear combination} of vectors of $S$ if there exist a finite number of vectors $u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$ in $F$ such that $v = a_1u_1 + a_2u_2 + \dots + a_nu_n$. In this case we also say that $v$ is a linear combination of $u_1, u_2, \dots, u_n$ and call $a_1, a_2, \dots, a_n$ the \textbf{coefficients} of the linear combination.
\end{definition}

\begin{definition}
	Let $S$ be a nonempty subset of a vector space $V$. The \textbf{span} of S, denoted $\text{span}(S)$, is the set consisting of all linear combinations of the vectors in $S$. For convenience, we define $\text{span}(\emptyset) = \left\{0\right\}$.
\end{definition}

\begin{theorem}
	The span of any subset $S$ of a vector space $V$ is a subspace of $V$ that contains $S$. Moreover, any subspace of $V$ that contains $S$ must also contain the span of $S$.
\end{theorem}

\begin{definition}
	A subset $S$ of a vector space $V$ \textbf{generates} (or \textbf{spans}) $V$ if $\text{span}(S) = V$. In this case, we also say that the vectors of $S$ generate (or span) $V$.
\end{definition}

\subsection*{Exercises}

\subsubsection*{1. Label the following statements as true or false}
\begin{enumerate}
	\item[(a)] The zero vector is a linear combination of any nonempty set of vectors - \textbf{True}.
	
	For any vectors $v_1, v_2, \dots, v_n$, $0 = 0v_1 + 0v_2 + \dots + 0v_n$.
	\item[(b)] The span of $\emptyset$ is $\emptyset$ - \textbf{False}.
	
	$\text{span}(\emptyset)$ is defined to be $\left\{0\right\}$.
	\item[(c)] If $S$ is a subset of a vector space $V$, then $\text{span}(S)$ equals the intersection of all subspaces of $V$ that contain $S$ - \textbf{True}.
	
	It follows from theorem 1.5.
	\item[(d)] In solving a system of linear equations, it is permissible to multiply an equation by any constant - \textbf{False}.
	
	It is permissible to multiply an equation by any nonzero constant.
	\item[(e)] In solving a system of linear equations, it is permissible to add any multiple of one equation to another - \textbf{True}.
	\item[(f)] Every system of linear equations has a solution - \textbf{False}.
	
	Any system of linear equations that, while solving, produces a system containing $0 = c$, where $c$ is nonzero, does not have a solution.
\end{enumerate}

\subsubsection*{7. In $F^n$, let $e_j$ denote the vector whose $j$th coordinate is 1 and whose other coordinates are 0. Prove that $e_1, e_2, \dots, e_n$ generates $F^n$.}
\begin{proof}
	Let $a$ be a vector in $F^n$, so $a = \left(a_1, a_2, \dots, a_n\right)$. Then \[ \left(a_1, a_2, \dots, a_n\right) = a_1e_1 + a_2e_2 + \dots + a_ne_n = (a_1, 0, \dots, 0) + (0, a_2, \dots, 0) + \dots + (0, 0, \dots, a_n) \]
\end{proof}

\subsubsection*{8. Show that $P_n(F)$ is generated by $\left\{1, x, \dots, x^n\right\}$.}
\begin{proof}
	Let $a$ be a vector in $P^n(F)$. Then $a = a_nx^n + a_{n-1}x^{n-1} + \dots + a_0 * 1$, so $P_n(F)$ is generated by $\left\{1, x, \dots, x^n\right\}$
\end{proof}

\subsubsection*{9. Show that the matrices \[
	\begin{bmatrix}
		1 & 0 \\ 0 & 0
	\end{bmatrix},
	\begin{bmatrix}
		0 & 1 \\ 0 & 0
	\end{bmatrix},
	\begin{bmatrix}
		0 & 0 \\ 1 & 0
	\end{bmatrix}\text{, and }
	\begin{bmatrix}
		0 & 0 \\ 0 & 1
	\end{bmatrix}
\] generate $M_{2 \times 2}(F)$.}
\begin{proof}
	Let $A \in M_{2 \times 2}(F)$. Then
	\[
		A =
		\begin{bmatrix}
			a_{11} & a_{12} \\ a_{21} & a_{22}
		\end{bmatrix} = 
		a_{11}\begin{bmatrix}
			1 & 0 \\ 0 & 0
		\end{bmatrix} + a_{12}\begin{bmatrix}
			0 & 1 \\ 0 & 0
		\end{bmatrix} + a_{21}\begin{bmatrix}
			0 & 0 \\ 1 & 0
		\end{bmatrix} + a_{22}\begin{bmatrix}
			0 & 0 \\ 0 & 1
		\end{bmatrix}.
	\]
\end{proof}

\subsubsection*{10. Show that if \[
	M_1 = \begin{bmatrix}
		1 & 0 \\ 0 & 0
	\end{bmatrix}, M_2 = \begin{bmatrix}
		0 & 0 \\ 0 & 1
	\end{bmatrix}\text{, and } M_3 = \begin{bmatrix}
		0 & 1 \\ 1 & 0
	\end{bmatrix},
\] then the span of $\left\{M_1, M_2, M_3\right\}$ is the set of all symmetric $2 \times 2$ matrices.}
\begin{proof}
	Let $A$ be a symmetric $2 \times 2$ matrix. Then \[
		A = \begin{bmatrix}
			a & b \\ b & c
		\end{bmatrix} = \begin{bmatrix}
			a & 0 \\ 0 & 0
		\end{bmatrix} + \begin{bmatrix}
			0 & b \\ b & 0
		\end{bmatrix} + \begin{bmatrix}
			0 & 0 \\ 0 & c
		\end{bmatrix} = aM_1 + bM_3 + cM_2
	\].
\end{proof}

\subsubsection*{11. Prove that $\text{span}(\left\{x\right\}) = \left\{ax: a \in F\right\}$ for any vector $x$ in a vector space. Interpret this result geometrically in $R^3$.}
\begin{proof}
	Let $x$ be a vector in some vector space over a field $F$, and let $y \in \text{span}(\left\{x\right\})$. Then $y$ is a linear combination of $x$, so $y = ax$ for some $a \in F$. Let $z \in \left\{ax: a \in F\right\}$. Then $z = ax$, so $z$ is a linear combination of $x$, so $z \in \text{span}(\left\{x\right\})$. $\text{span}(\left\{x\right\})$ in $R^3$ is a line going through $x$ and the origin.
\end{proof}

\subsubsection*{12. Show that a subset $W$ of a vector space $V$ is a subspace of $V$ if and only if $\text{span}(W) = W$.}
\begin{proof}
	Let $W$ be a subset of a vector space $V$, and that $\text{span}(W) = W$. Then $0 \in W$, since $0x = 0$ for any $x \in W$. Let $x$ and $y$ be elements of $W$. Then $x + y \in \text{span}(W)$, since its a linear combination of elements of $W$, and therefore $x + y \in W$. For any scalar $c$, $cx \in \text{span}(W)$, so $cx \in W$, which means that $W$ is a subspace of $V$.

	Now suppose that $W$ is a subspace of $V$. Then for vectors $x_1, x_2, \dots, x_n \in W$, and scalars $a_1, a_2, \dots, a_n$, $a_1x_1 + a_2x_2 + \dots + a_nx_n \in W$, so $W = \text{span}(W)$.
\end{proof}

\subsubsection*{13. Show that if $S_1$ and $S_2$ are subsets of a vector space $V$ such that $S_1 \subseteq S_2$ then $\text{span}(S_1) \subseteq \text{span}(S_2)$. In particular, if $S_1 \subseteq S_2$ and $\text{span}(S_1) = V$, deduce that $\text{span}(S_2) = V$.}
\begin{proof}
	Let $S_1$ and $S_2$ be subsets of a vector space $V$, such that $S_1 \subseteq S_2$. Then for all elements $x_1, x_2, \dots, x_n \in S_1$, we know that $x_1, x_2, \dots, x_n \in S_2$, so $\text{span}(S_2)$ contains all linear combinations $a_1x_1, a_2x_2, \dots, a_nx_n$, with $a_1, a_2, \dots, a_n$ being scalars, that is $\text{span}(S_1) \subseteq \text{span}(S_2)$.

	Now suppose that $S_1 \subseteq S_2$ and $\text{span}(S_1) = V$. Then $\text{span}(S_1) = V \subseteq \text{span}(S_2)$. Suppose that $\text{span}(S_2) \ne V$, that is, there exists some element $x \in \text{span}(S_2)$ such that $x \notin V$. Then $x$ is a linear combination of elements of $S_2$, and since $S_2$ is a subset of $V$, $x$ is a linear combination of some elements of $V$. Since $V$ is a subspace, this implies that $x \in V$, which leads to a contradiction.
\end{proof}

\subsubsection*{14. Show that if $S_1$ and $S_2$ are arbitrary subsets of a vector space $V$, then $\text{span}(S_1 \cup S_2) = \text{span}(S_1) + \text{span}(S_2)$.}
\begin{proof}
	Let $S_1$ and $S_2$ be arbitrary subsets of a vector space $V$. For a vector $v \in \text{span}(S_1 \cup S_2)$, we have $v = a_1x_1 + a_2x_2 + \dots a_nx_n + b_1y_1 + \dots b_my_m$, with $x_1, x_2, \dots, x_n \in S_1$, $y_1, y_2, \dots, y_m \in S_2$ and scalars $a_1, a_2, \dots, a_n$ and $b_1, b_2, \dots, b_m$. Then \[a_1x_1 + a_2x_2 + \dots a_nx_n \in \text{span}(S_1)\text{ and }b_1y_1 + b_2y_2 + \dots + b_my_m \in \text{span}(S_2),\] so for $x = a_1x_1 + a_2x_2 + \dots a_nx_n$ and $y = b_1y_1 + b_2y_2 + \dots + b_mx_m$, we have \[ v = x + y\text{, where } x\in \text{span}(S_1), y\in \text{span}(S_2), \] so $v \in \text{span}(S_1) + \text{span}(S_2)$.
	
	Now let $v \in \text{span}(S_1) + \text{span}(S_2)$. Then $v = x + y$, with $x \in \text{span}(S_1)$ and $y \in \text{span}(S_2)$. Then \[x = a_1x_1 + a_2x_2 + \dots + a_nx_n \text{ with } x_1, x_2, \dots, x_n \in S_1 \] and \[y = b_1y_1 + b_2y_2 + \dots + b_my_m \text{ with } y_1, y_2, \dots, y_m \in S_2.\] Then $v$ is a linear combination of vectors in $S_1$ and vectors in $S_2$, which means its a linear combination of vector in $S_1 \cup S_2$, so $v \in \text{span}(S_1 \cup S_2)$.
\end{proof}

\subsubsection*{16. Let $V$ be a vector space and $S$ a subset of $V$ with the property that whenever $v_1, v_2, \dots, v_n \in S$ and $a_1v_1 + a_2v_2 + \dots + a_nv_n = 0$, then $a_1 = a_2 = \dots = a_n = 0$. Prove that every vector in the span of $S$ can be uniquely written as a linear combination of vectors of S.}
\begin{proof}
	Suppose that for a vector $x \in \text{span}(S)$, $x$ can be written as two different linear combinations of vectors of $S$, that is, for $x_1, x_2, \dots, x_n \in S$ and for scalars $a_1, b_1, a_2, b_2, \dots, a_n, b_n$, \[ a_1x_1 + a_2x_2 + \dots + a_nx_n = b_1x_1 + b_2x_2 + \dots + b_nx_n \text{ and } a_i \ne b_i \text{ for } 1 \le i \le n. \] Then
	\begin{align*}
		a_1x_1 + a_2x_2 + \dots + a_nx_n - b_1x_1 - b_2x_2 - \dots b_nx_n = (a_1 - b_1)x_1 + (a_2 - b_2)x_2 + \dots (a_n - b_n)x_n = 0,
	\end{align*}
	so $(a_1 - b_1) = (a_2 - b_2) = \dots = (a_n - b_n) = 0$, so $a_i = b_i$, for $1 \le i \le n$.
\end{proof}

\section{Linear Dependence and Linear Independence}
\begin{definition}
	A subset $S$ of a vector space $V$ is called \textbf{linearly dependent} if there exist a finite number of distinct vectors $u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$ not all zero, such that \[a_1u_1 + a_2u_2 + \dots + a_nu_n = 0\]In this case we also say that the vectors of $S$ are linearly dependent.
\end{definition}

If for any vectors $u_1, u_2, \dots, u_n$ we have $a_1u_1 + a_2u_2 + \dots + a_nu_n = 0$ if $a_1 = a_2 = \dots = a_n = 0$, we call this the \textbf{trivial representation} of $0$ as a linear combination of $u_1, u_2, \dots, u_n$.

\begin{definition}
	A subset $S$ of a vector space that is not linearly dependent is called \textbf{linearly independent}. As before, we also say that the vectors of $S$ are linearly independent.
\end{definition}

\begin{theorem}
	Let $V$ be a vector space, and let $S_1 \subseteq S_2 \subseteq V$. If $S_1$ is linearly dependent, then $S_2$ is linearly dependent.
\end{theorem}
\begin{corollary}
	Let $V$ be a vector space, and let $S_1 \subseteq S_2 \subseteq V$. If $S_2$ is linearly independent, then $S_1$ is linearly independent.
\end{corollary}

\begin{theorem}
	Let $S$ be a linearly independent subset of a vector space $V$, and let $v$ be a vector in $V$ that is not in $S$. Then $S \cup \left\{v\right\}$ is linearly dependent if and only if $v \in \text{span}(S)$.
\end{theorem}

\subsection*{Exercises}

\subsubsection*{1. Label the following statements as true or false.}
\begin{enumerate}
	\item[(a)] If $S$ is a linearly dependent set, then each vector in $S$ is a linear combination of other vectors in $S$ - \textbf{False}.
	
	At least one vector in $S$ is a linear combination of other vectors in $S$.
	\item[(b)] Any set containing the zero vector is linearly dependent - \textbf{True}.
	
	$0 = 0v_1 + 0v_2 + \dots + 0v_n$, so $0$ is a linear combination of other vectors.
	\item[(c)] The empty set is linearly dependent - \textbf{False}.
	
	Linearly dependent sets must be nonempty.
	\item[(d)] Subsets of linearly dependent sets are linearly dependent - \textbf{False}.
	
	Consider $S_1 = \left\{(1, 0), (0, 1), (0, 2)\right\}$ and $S_2 = \left\{(1, 0), (0, 1)\right\}$. Then $S_1$ is linearly dependent, $S_2 \subset S_1$, and $S_2$ is linearly independent.
	\item[(e)] Subsets of linearly independent sets are linearly dependent - \textbf{True}.
	
	Follows from theorem 1.6.
	\item[(f)] If $a_1x_1 + a_2x_2 + \dots + a_nx_n = 0$ and $x_1, x_2, \dots, x_n$ are linearly independent, then all the scalars $a_i$ are zero - \textbf{True}.
\end{enumerate}

\subsubsection*{4. In $F^n$, let $e_j$ denote the vector whose $j$th coordinate is $1$ and whose other coordinates are $0$. Prove that $\left\{e_1, e_2, \dots, e_n\right\}$ is linearly independent.}
\begin{proof}
	\[a_1e_1 + a_2e_2 + \dots + a_ne_n = \left(a_1, a_2, \dots, a_n\right),\] so it is equal $0$ only when $a_1 = a_2 = \dots = a_n = 0$, which means $\left\{e_1, e_2, \dots, e_n\right\}$ is linearly independent.
\end{proof}

\subsubsection*{5. Show that the set $\left\{1, x, x^2, \dots, x^n\right\}$ is linearly independent in $P_n(F)$.}
\begin{proof}
	If \[a_0 + a_1x + a_2x^2 + \dots + a_nx^n = 0\]for some scalars $a_0, a_1, \dots, a_n$, then this is the unique $0$ vector in $P_n(F)$, so $a_0 = a_1 = \dots = a_n = 0$.
\end{proof}

\subsubsection*{6. In $M_{3\times2}(F)$, prove that the set \[\left\{\begin{bmatrix}
	1 & 1 \\ 0 & 0 \\ 0 & 0
\end{bmatrix}, \begin{bmatrix}
	0 & 0 \\ 1 & 1 \\ 0 & 0
\end{bmatrix}, \begin{bmatrix}
	0 & 0 \\ 0 & 0 \\ 1 & 1
\end{bmatrix}, \begin{bmatrix}
	1 & 0 \\ 1 & 0 \\ 1 & 0
\end{bmatrix}, \begin{bmatrix}
	0 & 1 \\ 0 & 1 \\ 0 & 1
\end{bmatrix}\right\}\] is linearly dependent.}
\begin{proof}
	\[
		\begin{bmatrix}
			1 & 1 \\ 0 & 0 \\ 0 & 0
		\end{bmatrix} + \begin{bmatrix}
			0 & 0 \\ 1 & 1 \\ 0 & 0
		\end{bmatrix} + \begin{bmatrix}
			0 & 0 \\ 0 & 0 \\ 1 & 1
		\end{bmatrix} - \begin{bmatrix}
			1 & 0 \\ 1 & 0 \\ 1 & 0
		\end{bmatrix} - \begin{bmatrix}
			0 & 1 \\ 0 & 1 \\ 0 & 1
		\end{bmatrix} = 0
	\] so the set is linearly dependent.
\end{proof}

\subsubsection*{8. Let $S = \left\{(1, 1, 0), (1, 0, 1), (0, 1, 1)\right\}$ be a subset of the vector space $F^3$.}
\begin{enumerate}
	\item[(a)] Prove that if $F = \R$, then $S$ is linearly independent.
	\begin{proof}
		If \[a_1(1, 1, 0) + a_2(1, 0, 1) + a_3(0, 1, 1) = 0\] then \[a_1 + a_2 = 0\]\[a_1 + a_3 = 0\]\[a_2 + a_3 = 0\] so $a_1 = a_2 = a_3 = 0$.
	\end{proof}
	\item[(b)] Prove that if $F$ has characteristic two, then $S$ is linearly dependent.
	\begin{proof}
		If $F$ has characteristic two, then \[(1, 1, 0) + (1, 0, 1) + (0, 1, 1) = (1 + 1, 1 + 1, 1+ 1) = 0\]so $S$ is linearly dependent.
	\end{proof}
\end{enumerate}

\subsubsection*{9. Let $u$ and $v$ be distinct vectors in a vector space $V$. Show that $\left\{u, v\right\}$ is linearly dependent if and only if $u$ or $v$ is a multiple of the other.}
\begin{proof}
	Suppose that $\left\{u, v\right\}$ is linearly dependent. Then for some scalars $a_1, a_2$ not both $0$, \[a_1u + a_2v = 0,\] so $a_1u = -a_2v$. Suppose without loss of generality that $a_1 \ne 0$. Then \[u = -\frac{a_2}{a_1}v,\] so $v$ is a multiple of $u$.
	
	For the converse, suppose without loss of generality that $v$ is a multiple of $u$. Then for some scalar $a$, $u = av$, so $u - av = 0$, which means that $\left\{u, v\right\}$ is linearly dependent.
\end{proof}

\subsubsection*{10. Give an example of three linearly dependent vectors in $\R^3$ such that none of the three is a multiple of another.}
$(1, 0, 0), (0, 1, 0), (1, 1, 0)$

\subsubsection*{12. Prove Theorem 1.6 and its corollary.}
\begin{proof}
	Let $V$ be a vector space, and let $S_1 \subseteq S_2 \subseteq V$, and suppose that $S_1$ is linearly dependent. That means, there exists a vector $x_0 \in S_1$ that can be represented as a linear combination of some other vectors in $S_1$: $x_1, x_2, \dots, x_n$. So \[x_0 = a_1x_1 + a_2x_2 + \dots + a_nx_n.\] Then since $x_0, x_1, \dots, x_n$ are all in $S_2$, there exists a vector in $S_2$ that can be represented as a linear combination of some other vectors in $S_2$, so $S_2$ is linearly dependent.
\end{proof}
\begin{corollary}
	Let $V$ be a vector space, and let $S_1 \subseteq S_2 \subseteq V$. If $S_2$ is linearly independent, then $S_1$ is linearly independent.
\end{corollary}
\begin{proof}
	This is just the contraposition of Theorem 1.6.
\end{proof}

\subsubsection*{14. Prove that a set $S$ is linearly dependent if and only if $S = \left\{0\right\}$ or there exist distinct vectors $v, u_1, u_2, \dots, u_n$ such that $v$ is a linear combination of $u_1, u_2, \dots, u_n$.}
\begin{proof}
	Let $S = \left\{0\right\}$. Then $S$ is linearly dependent, because $0 = a \cdot 0$ for any non-zero scalar $a$. Suppose that there exist distinct vectors $v, u_1, u_2, \dots, u_n$ such that $v$ is a linear combination of $u_1, u_2, \dots, u_n$, so \[v = a_1u_1 + a_2u_2 + \dots + a_nu_n.\] Then \[0 = a_1u_1 + a_2u_2 + \dots a_nu_n - v,\] so $S$ is linearly dependent.

	For the converse, suppose that $S \ne \left\{0\right\}$ and that there are no distinct vectors $v, u_1, u_2, \dots, u_n$ such that $v$ is a linear combination of $u_1, u_2, \dots, u_n$. Suppose that $S$ is linearly dependent. Then there exist scalars $a_0, a_1, \dots, a_n$ not all $0$ such that \[0 = a_0v + a_1u_1 + a_2u_2 + \dots + a_nu_n.\] Then at least $1$ scalar is not $0$, say $a_0$. Then \[-a_0v = a_1u_1 + a_2u_2 + \dots + a_nu_n,\] so \[v = -\frac{1}{a_0}(a_1u_1 + a_2u_2 + \dots + a_nu_n)\] which contradicts the original assumption, so $S$ has to be linearly independent.
\end{proof}

\subsubsection*{15. Let $S = \left\{u_1. u_2, \dots, u_n\right\}$ be a finite set of vectors. Prove that $S$ is linearly dependent if and only if $u_1 = 0$ or $u_{k+1}\in \text{span}(\left\{u_1, u_2, \dots, u_k\right\})$ for some $k$ ($1 \le k < n$).}
\begin{proof}
	Let $S = \left\{u_1, u_2, \dots, u_n\right\}$ be a finite set of vectors. If $u_1 = 0$ then \[0 = au_1 + 0u_2 + 0u_3 + \dots + 0u_n\] with $a$ nonzero, so $S$ is linearly dependent. Suppose that for some $k$, $1 \le k < n$, $u_{k+1} \in \text{span}(\left\{u_1, u_2, \dots, u_k\right\})$. Then there exist some scalars $a_1, a_2, \dots, a_k$ such that \[u_{k+1} = a_1u_1 + a_2u_2 + \dots + a_ku_k,\] so \[0 = a_1u_1 + a_2u_2 + \dots + a_ku_k - u_{k+1}\] so $S$ is linearly dependent.

	For the converse, suppose that $u_1 \ne 0$ and that there is no $k$ such that $1 \le k < n$ and $u_{k+1} \in \text{span}(\left\{u_1, u_2, \dots, u_k\right\})$. Suppose that $S$ is linearly dependent. Then \[0 = a_1u_1 + a_2u_2 + \dots + a_nu_n\] with scalars $a_1, a_2, \dots, a_n$ not all $0$. Then there exists a nonzero scalar such that the scalars after it in the above sum are all $0$, say that scalar is $a_n$. Then \[u_n = -\frac{1}{a_n}(a_1u_1 + a_2u_2 + \dots + a_{n-1}u_{n-1}).\] So $u_n \in \text{span}(\left\{u_1, u_2, \dots, u_{n-1}\right\})$ which contradicts the original assumption, so $S$ has to be linearly independent.
\end{proof}

\subsubsection*{16. Prove that a set $S$ of vectors is linearly independent if and only if each finite subset of $S$ is linearly independent.}
\begin{proof}
	Suppose that a set $S$ of vectors is linearly independent. Then it follows from the corollary of Theorem 1.6 that any finite subset of $S$ has to be linearly independent.

	For the converse, suppose that each finite subset of $S$ is linearly independent. Then there is no finite number of distinct vectors $u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$ not all zero, such that \[a_1u_1 + a_2u_2 + \dots + a_nu_n = 0,\] which is the definition of linear independence.
\end{proof}

\subsubsection*{20. Let $f, g \in F(\R, \R)$ be the functions defined by $f(t) = e^{rt}$ and $g(t) = e^{st}$, where $r \ne s$. Prove that $f$ and $g$ are linearly independent in $F(\R, \R)$.}
\begin{proof}
	Suppose that $f$ and $g$ are linearly dependent. Then $f = ig$ for some $i$. So $f(0) = i \cdot g(0)$, and $1 = i \cdot 1$, so $i = 1$. We then have $f(1) = g(1)$, so $e^r = e^s$, and $r = s$ which contradicts $r \ne s$.
\end{proof}

\subsubsection*{21. Let $S_1$ and $S_2$ be disjoint linearly independent subsets of $V$. Prove that $S_1 \cup S_2$ is linearly dependent if and only if $\text{span}(S_1) \cap \text{span}(S_2) \ne \left\{0\right\}$.}
\begin{proof}
	Suppose that $S_1 \cup S_2$ is linearly dependent. Then there exist vectors $u_1, u_2, \dots, u_n \in S_1$, and $v_1, v_2, \dots, v_m \in S_2$ and scalars $a_1, \dots, a_n, b_1, \dots, b_m$ not all zero such that \[ a_1u_1 + a_2u_2 + \dots + a_nu_n + b_1v_1 + b_2v_2 + \dots + b_mv_m = 0. \]Then \[a_1u_1 + a_2u_2 + \dots + a_nu_n = -b_1v_1 - b_2v_2 - \dots - b_mv_m,\]and since $S_1$ and $S_2$ are linearly independent, we know these are not equal to 0, so it is a non-zero element of $\text{span}(S_1)\cap \text{span}(S_2)$.

	For the converse, suppose that $\text{span}(S_1)\cap\text{span}(S_2)\ne\left\{0\right\}.$ Then there exists a non-zero element \[x = a_1u_1 + a_2u_2 + \dots + a_nu_n = b_1v_1 + b_2v_2 + \dots + b_mv_m\] for some vectors $u_1, u_2, \dots, u_n \in S_1$ and $v_1, v_2, \dots, v_m \in S_2$ and vectors $a_1, a_2, \dots, a_n, b_1, b_2, \dots, b_m$. Then \[ a_1u_1 + a_2u_2 + \dots + a_nu_n - b_1v_1 - b_2v_2 - \dots - b_mv_m = 0,\] so $S_1 \cup S_2$ is linearly dependent.
\end{proof}

\section{Bases and Dimension}

\begin{definition}
    A \textbf{basis} $\beta$ for a vector space $\textbf{V}$ is a linearly independent subset of $\textbf{V}$ that generates $\textbf{V}$. If $\beta$ is a basis for $\textbf{V}$, we also say that the vectors of $\beta$ form a basis for $\textbf{V}$.
\end{definition}

\begin{theorem}
    Let $\textbf{V}$ be a vector space and $u_1, u_2, \dots, u_n$ be distinct vectors in $\textbf{V}$. Then $\beta = \{u_1, u_2, \dots, u_n\}$ is a basis for $\textbf{V}$ if and only if each $v\in\textbf{V}$ can be uniquely expressed as a linear combination of vectors of $\beta$, that is, can be expressed in the form \[v = a_1u_1 + a_2u_2 + \dots + a_nu_n\] for unique scalars $a_1, a_2, \dots, a_n$.
\end{theorem}

\begin{theorem}
    If a vector space $\textbf{V}$ is generated by a finite set $S$, then some subset of $S$ is a basis for $\textbf{V}$. Hence $\textbf{V}$ has a finite basis.
\end{theorem}

\begin{theorem}
    $\textbf{(Replacement Theorem.)}$ Let $\textbf{V}$ be a vector space that is generated by a set $G$ containing exactly $n$ vectors, and let $L$ be a linearly independent subset of $\textbf{V}$ containing exactly $m$ vectors. Then $m \le n$ and there exists a subset $H$ of $G$ containing exactly $n-m$ vectors such that $L \cup H$ generates $\textbf{V}$.
\end{theorem}

\begin{corollary}
    Let $\textbf{V}$ be a vector space having a finite basis. Then all bases for $\textbf{V}$ are finite, and every basis for $\textbf{V}$ contains the same number of vectors.
\end{corollary}

\begin{definition}
    A vector space is called $\textbf{finite-dimensional}$ if it has a basis consisting of a finite number of vectors. The unique integer $n$ such that every basis for $\textbf{V}$ contains exactly $n$ elements is called the $\textbf{dimension}$ of $\textbf{V}$ and is denoted by $dim(\textbf{V})$. A vector space that is not finite-dimensional is called $\textbf{infinite-dimensional}$.
\end{definition}

\begin{corollary}
    Let $\textbf{V}$ be a vector space with dimension $n$.
    \begin{enumerate}
        \item[(a)] Any finite generating set for $\textbf{V}$ contains at least $n$ vectors, and a generating set for $\textbf{V}$ that contains exactly $n$ vectors is a basis for $\textbf{V}$.
        \item[(b)] Any linearly independent subset of $\textbf{V}$ that contains exactly $n$ vectors is a basis for $\textbf{V}$.
        \item[(c)] Every linearly independent subset of $\textbf{V}$ can be extended to a basis for $\textbf{V}$, that is, if $L$ is a linearly independent subset of $\textbf{V}$, then there is a basis $\beta$ of $\textbf{V}$ such that $L\subseteq \beta$.
    \end{enumerate}
\end{corollary}

\begin{theorem}
    Let $W$ be a subspace of a finite-dimensional vector space $V$. Then $W$ is finite-dimensional and $dim(W)\le dim(V)$. Moreover, if $dim(W)=dim(V)$, then $V=W$.
\end{theorem}

\begin{corollary}
    If $W$ is a subspace of a finite-dimensional vector space $V$, then any basis for $W$ can be extended to a basis for $V$.
\end{corollary}

\subsection{Exercises}

\subsubsection*{1. Label the following statements as true or false}
\begin{enumerate}
    \item[(a)] The zero vector space has no basis - \textbf{False}.

    The basis for the zero vector space is $\emptyset$.
    \item[(b)] Every vector space that is generated by a finite set has a basis - \textbf{True}.
    \item[(c)] Every vector space has a finite basis - \textbf{False}.

    Consider $P(F)$ for which the basis is $\{1,x,x^2,\dots\}$.
    \item[(d)] A vector space cannot have more than one basis - \textbf{False}.
    
    Consider $R$ where both $\{1\}$ and $\{-1\}$ are a basis.
    \item[(e)] If a vector space has a finite basis, then the number of vectors in every basis is the same - \textbf{True}.

    It follows from corollary 1 of the replacement theorem.
    \item[(f)] The dimension of $P_n(F)$ is $n$ - \textbf{False}.

    The dimension of $P_n(F)$ is $n + 1$.
    \item[(g)] The dimension of $M_{m\times n}(F)$ is $m + n$ - \textbf{False}.

    The dimension of $M_{m\times n}(F)$ is $m \times n$.
    \item[(h)] Suppose that $V$ is a finite-dimensional vector space, that $S_1$ is a linearly independent subset of $V$, and that $S_2$ is a subset of $V$ that generates $V$. Then $S_1$ cannot contain more vectors than $S_2$ - \textbf{True}.

    This is stated in the replacement theorem.
    \item[(i)] If $S$ generates the vector space $V$, then every vector in $V$ can be written as a linear combination of vectors in S in only one way - \textbf{False}.

    This is only true if $S$ is linearly independent.
    \item[(j)] Every subspace of a finite-dimensional space is finite-dimensional - \textbf{True}.

    It follows from theorem 1.11.
    \item[(k)] If $V$ is a vector space having dimension $n$, then $V$ has exactly one subspace with dimension 0 and exactly one subspace with dimension $n$ - \textbf{True}.

    It follows from theorem 1.11.
    \item[(l)] If $V$ is a vector space having dimension $n$, and if $S$ is a subset of $V$ with $n$ vectors, then $S$ is linearly independent if and only if $S$ spans $V$ - \textbf{True}.

    It follows from corollary 2 of the replacement theorem.
\end{enumerate}

\subsubsection*{6. Give three different bases for $F^2$ and $M_{2\times 2}(F)$}

Let $a \in F$ and $a \ne 0$. Then $A = \left\{(a, 0), (0, a)\right\}$, $B = \left\{(-a, 0), (0, -a)\right\}$ and $C \left\{(a, 0), (0, -a)\right\}$ are all bases for $F^2$.

Then $D = \left\{
	\begin{bmatrix}
		a & 0 \\
		0 & 0
	\end{bmatrix},
	\begin{bmatrix}
		0 & a \\
		0 & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 \\
		a & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 \\
		0 & a
	\end{bmatrix}
\right\}$,
$E = \left\{
	\begin{bmatrix}
		-a & 0 \\
		0 & 0
	\end{bmatrix},
	\begin{bmatrix}
		0 & -a \\
		0 & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 \\
		-a & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 \\
		0 & -a
	\end{bmatrix}
\right\}$, and
$F = \left\{
	\begin{bmatrix}
		a & 0 \\
		0 & 0
	\end{bmatrix},
	\begin{bmatrix}
		0 & a \\
		0 & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 \\
		-a & 0
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 \\
		0 & -a
	\end{bmatrix}
\right\}$ are all bases for $M_{2\times 2}(F)$.

\subsubsection*{9. The vectors $u_1 = (1,1,1,1), u_2=(0,1,1,1), u_3=(0,0,1,1)$ and $u_4 = (0,0,0,1)$ form a basis for $F^4$. Find the unique representation of an arbitrary vector $(a_1, a_2, a_3, a_4)$ in $F^4$ as a linear combination of $u_1, u_2, u_3, u_4$.}

\begin{align*}
	a_1(1,1,1,1) &+ (a_2 - a_1)(0, 1, 1, 1) + (a_3 - a_2)(0, 0, 1, 1) + (a_4 - a_3)(0, 0, 0, 1) \\&= (a_1, a_1 + a_2 - a_1, a_1 + a_2 - a_1 + a_3 - a_2, a_1 + a_2 - a_1 + a_3 - a_2 + a_4 - a_3) \\&= (a_1, a_2, a_3, a_4) 
\end{align*}

\subsubsection*{11. Let $u$ and $v$ be distinct vectors of a vector space $V$. Show that if $\left\{u, v\right\}$ is a basis for $V$ and $a$ and $b$ are nonzero scalars, then both $\left\{u + v, au\right\}$ and $\left\{au, bv\right\}$ are also bases for $V$.}

\begin{proof}
	Let $w$ be a vector in $V$. Then $w = xu + yv$ for some scalars $x$ and $y$. Then \[xu + yv = yu + yv + xu - yu = y(u + v) + \frac{x - y}{a}(au),\] so $A = \left\{u + v, au\right\}$ generates $V$.

	Additionally, \[xu + yu = \frac{x}{a}au + \frac{y}{b}bv,\] so $B = \left\{au, bv\right\}$ also generates $V$.
	
	Since $dim(V) = 2$ and both $A$ and $B$ are generating sets for $V$, each with 2 vectors, they are both a basis for $V$.
\end{proof}

\subsubsection*{15. The set of all $n\times n$ matrices having trace equal to zero is a subspace $W$ of $M_{n\times n}(F)$. Find a basis for $W$. What is the dimension of $W$?}

\end{document}
